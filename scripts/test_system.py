#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Sales Agent —Å–∏—Å—Ç–µ–º—ã.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –æ—Ç—á–µ—Ç–∞–º–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º VRAM.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import asyncio
import argparse
import json
import time
import traceback
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
import httpx
import psutil

try:
    import torch
    import pynvml
    pynvml.nvmlInit()
    NVML_AVAILABLE = True
except (ImportError, OSError):
    NVML_AVAILABLE = False

from src.shared.health import HealthChecker


@dataclass
class TestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞."""
    name: str
    status: str  # "success", "error", "warning"
    duration_seconds: float
    vram_before_mb: Optional[float] = None
    vram_after_mb: Optional[float] = None
    vram_peak_mb: Optional[float] = None
    metrics: Dict[str, Any] = None
    error: Optional[str] = None
    warnings: List[str] = None
    
    def __post_init__(self):
        if self.metrics is None:
            self.metrics = {}
        if self.warnings is None:
            self.warnings = []


class SystemTester:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã."""
    
    def __init__(self, stop_on_error: bool = True, continue_on_error: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞.
        
        Args:
            stop_on_error: –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –ø—Ä–∏ –æ—à–∏–±–∫–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
            continue_on_error: –ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç stop_on_error)
        """
        self.results: Dict[str, TestResult] = {}
        self.vram_snapshots: List[Dict[str, Any]] = []
        self.stop_on_error = stop_on_error and not continue_on_error
        self.start_time = time.time()
        
    def get_vram_usage(self) -> Optional[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –≤ MB."""
        if not NVML_AVAILABLE:
            try:
                # Fallback —á–µ—Ä–µ–∑ torch
                if torch.cuda.is_available():
                    return torch.cuda.memory_allocated(0) / (1024 ** 2)
            except:
                pass
            return None
        
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            return info.used / (1024 ** 2)
        except:
            return None
    
    def run_step(self, step_name: str, step_func: Callable) -> bool:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫.
        
        Args:
            step_name: –ò–º—è —à–∞–≥–∞
            step_func: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ —à–∞–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        print(f"\n{'='*70}")
        print(f"üîç {step_name}")
        print(f"{'='*70}")
        
        vram_before = self.get_vram_usage()
        start_time = time.time()
        result = TestResult(name=step_name, status="success", duration_seconds=0.0)
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥
            if asyncio.iscoroutinefunction(step_func):
                step_result = asyncio.run(step_func())
            else:
                step_result = step_func()
            
            duration = time.time() - start_time
            vram_after = self.get_vram_usage()
            
            result.duration_seconds = duration
            result.vram_before_mb = vram_before
            result.vram_after_mb = vram_after
            
            if isinstance(step_result, dict):
                result.metrics.update(step_result)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º snapshot VRAM
            self.vram_snapshots.append({
                "step": step_name,
                "timestamp": datetime.now().isoformat(),
                "vram_mb": vram_after,
                "vram_delta_mb": (vram_after - vram_before) if vram_before and vram_after else None,
            })
            
            print(f"‚úÖ {step_name} - —É—Å–ø–µ—à–Ω–æ ({duration:.2f}s)")
            if vram_before and vram_after:
                delta = vram_after - vram_before
                print(f"   VRAM: {vram_before:.0f} MB ‚Üí {vram_after:.0f} MB (Œî {delta:+.0f} MB)")
            
            result.status = "success"
            self.results[step_name] = result
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            vram_after = self.get_vram_usage()
            
            result.duration_seconds = duration
            result.status = "error"
            result.error = str(e)
            result.vram_before_mb = vram_before
            result.vram_after_mb = vram_after
            
            print(f"‚ùå {step_name} - –æ—à–∏–±–∫–∞ ({duration:.2f}s)")
            print(f"   –û—à–∏–±–∫–∞: {e}")
            print(f"   Traceback:")
            traceback.print_exc()
            
            self.results[step_name] = result
            
            if self.stop_on_error:
                print(f"\n‚ö†Ô∏è  –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –≤ —à–∞–≥–µ '{step_name}'")
                print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --continue-on-error —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö")
                return False
            
            return False
    
    def test_environment(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        metrics = {}
        
        # Python –≤–µ—Ä—Å–∏—è
        python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        metrics["python_version"] = python_version
        if sys.version_info < (3, 12):
            raise RuntimeError(f"–¢—Ä–µ–±—É–µ—Ç—Å—è Python 3.12+, —Ç–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è: {python_version}")
        print(f"   Python: {python_version} ‚úÖ")
        
        # CUDA
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        
        cuda_version = torch.version.cuda
        metrics["cuda_version"] = cuda_version
        print(f"   CUDA: {cuda_version} ‚úÖ")
        
        # GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        gpu_count = torch.cuda.device_count()
        metrics["gpu_count"] = gpu_count
        print(f"   GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {gpu_count}")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            total_vram_gb = props.total_memory / (1024 ** 3)
            metrics[f"gpu_{i}_name"] = props.name
            metrics[f"gpu_{i}_vram_gb"] = total_vram_gb
            print(f"   GPU {i}: {props.name}")
            print(f"      VRAM: {total_vram_gb:.2f} GB")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–∫–µ—Ç–æ–≤
        packages = {
            "torch": torch.__version__,
            "faster-whisper": None,
            "f5-tts": None,
            "vllm": None,
            "huggingface-hub": None,
        }
        
        try:
            import faster_whisper
            packages["faster-whisper"] = getattr(faster_whisper, "__version__", "unknown")
        except ImportError:
            raise RuntimeError("faster-whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        try:
            import f5_tts
            packages["f5-tts"] = getattr(f5_tts, "__version__", "unknown")
        except ImportError:
            raise RuntimeError("f5-tts –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        try:
            import vllm
            packages["vllm"] = getattr(vllm, "__version__", "unknown")
        except ImportError:
            # vllm –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
            packages["vllm"] = None
            print("   ‚ö†Ô∏è  vllm –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)")
        
        try:
            import huggingface_hub
            packages["huggingface-hub"] = getattr(huggingface_hub, "__version__", "unknown")
        except ImportError:
            pass  # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
        
        metrics["packages"] = packages
        print(f"   –ü–∞–∫–µ—Ç—ã: {', '.join(f'{k}={v}' for k, v in packages.items() if v)} ‚úÖ")
        
        return metrics
    
    def test_local_models(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∏—Å–∫–µ."""
        metrics = {}
        project_root = Path(__file__).parent.parent
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ F5-TTS
        f5_path = project_root / "models" / "F5-tts"
        if not f5_path.exists():
            raise RuntimeError(f"F5-TTS –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {f5_path}")
        
        f5_files = list(f5_path.glob("*.pt")) + list(f5_path.glob("*.safetensors"))
        if not f5_files:
            raise RuntimeError(f"–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ F5-TTS –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {f5_path}")
        
        f5_size_mb = sum(f.stat().st_size for f in f5_files) / (1024 ** 2)
        metrics["f5_tts_path"] = str(f5_path)
        metrics["f5_tts_size_mb"] = f5_size_mb
        metrics["f5_tts_files"] = [f.name for f in f5_files]
        print(f"   F5-TTS: {f5_path}")
        print(f"      –†–∞–∑–º–µ—Ä: {f5_size_mb:.2f} MB")
        print(f"      –§–∞–π–ª—ã: {', '.join(f.name for f in f5_files)} ‚úÖ")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Qwen3
        qwen_path = project_root / "models" / "Qwen3-16B-A3B-abliterated-AWQ"
        if not qwen_path.exists():
            raise RuntimeError(f"Qwen3 –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {qwen_path}")
        
        config_json = qwen_path / "config.json"
        if not config_json.exists():
            raise RuntimeError(f"config.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {qwen_path}")
        
        # –ü–æ–¥—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
        model_files = list(qwen_path.glob("*.safetensors")) + list(qwen_path.glob("*.bin"))
        if not model_files:
            raise RuntimeError(f"–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ Qwen3 –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {qwen_path}")
        
        qwen_size_gb = sum(f.stat().st_size for f in qwen_path.rglob("*") if f.is_file()) / (1024 ** 3)
        metrics["qwen_path"] = str(qwen_path)
        metrics["qwen_size_gb"] = qwen_size_gb
        metrics["qwen_model_files_count"] = len(model_files)
        print(f"   Qwen3: {qwen_path}")
        print(f"      –†–∞–∑–º–µ—Ä: {qwen_size_gb:.2f} GB")
        print(f"      –§–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏: {len(model_files)} ‚úÖ")
        
        return metrics
    
    def run_all_tests(self, steps: Optional[List[str]] = None):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ.
        
        Args:
            steps: –°–ø–∏—Å–æ–∫ —à–∞–≥–æ–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None, –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤—Å–µ)
        """
        all_steps = [
            ("environment", self.test_environment),
            ("local_models", self.test_local_models),
            ("asr_model", self.test_asr_model),
            ("tts_model", self.test_tts_model),
            ("llm_model", self.test_llm_model),
            ("vram_all_models", self.test_vram_all_models),
            ("redis", self.test_redis),
            ("vllm_server", self.test_vllm_server),
            ("asr_gateway", self.test_asr_gateway),
            ("tts_gateway", self.test_tts_gateway),
            ("policy_engine", self.test_policy_engine),
            ("freeswitch_bridge", self.test_freeswitch_bridge),
            ("e2e_dialog", self.test_e2e_dialog),
        ]
        
        if steps:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —à–∞–≥–∏
            step_dict = {name: func for name, func in all_steps}
            all_steps = [(name, step_dict[name]) for name in steps if name in step_dict]
        
        print("=" * 70)
        print(" " * 20 + "Sales Agent - System Test")
        print("=" * 70)
        print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"–®–∞–≥–æ–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {len(all_steps)}")
        print()
        
        for step_name, step_func in all_steps:
            success = self.run_step(step_name, step_func)
            if not success and self.stop_on_error:
                break
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        self.generate_report()
    
    def test_asr_model(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–µ—Å—Ç ASR –º–æ–¥–µ–ª–∏ (faster-whisper) —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º VRAM –∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏."""
        import numpy as np
        from faster_whisper import WhisperModel
        
        metrics = {}
        
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ faster-whisper large-v3-turbo (Dropbox)...")
        model = WhisperModel(
            "dropbox-dash/faster-whisper-large-v3-turbo",
            device="cuda",
            compute_type="int8_float16",
        )
        
        vram_after_load = self.get_vram_usage()
        metrics["vram_after_load_mb"] = vram_after_load
        
        # –¢–µ—Å—Ç–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        print("   –¢–µ—Å—Ç–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ...")
        test_audio = np.random.randn(16000).astype(np.float32)  # 1 —Å–µ–∫—É–Ω–¥–∞ @ 16kHz
        
        start_time = time.time()
        segments, info = model.transcribe(test_audio, beam_size=1)
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç
        first_segment = next(segments, None)
        latency_ms = (time.time() - start_time) * 1000
        
        metrics["latency_ms"] = latency_ms
        metrics["language"] = info.language if hasattr(info, 'language') else None
        
        print(f"   –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {latency_ms:.2f} –º—Å")
        
        # –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        del model
        torch.cuda.empty_cache()
        
        vram_after_unload = self.get_vram_usage()
        metrics["vram_after_unload_mb"] = vram_after_unload
        
        return metrics
    
    def test_tts_model(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ç–µ—Å—Ç TTS –º–æ–¥–µ–ª–∏ (F5-TTS) —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º VRAM –∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ first-audio."""
        from src.tts_gateway.f5_tts_engine import F5TTSEngine
        import logging
        
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.WARNING)
        
        project_root = Path(__file__).parent.parent
        model_path = project_root / "models" / "F5-tts"
        
        metrics = {}
        
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ F5-TTS –∏–∑ {model_path}...")
        f5_tts = F5TTSEngine(
            model_path=str(model_path),
            device="cuda",
            sample_rate=24000,
            use_stress_marks=True,
            logger=logger,
        )
        
        vram_after_load = self.get_vram_usage()
        metrics["vram_after_load_mb"] = vram_after_load
        
        # –¢–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑
        test_text = "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ö–∞–∫ —è –º–æ–≥—É –≤–∞–º –ø–æ–º–æ—á—å?"
        print(f"   –¢–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑: '{test_text}'...")
        
        start_time = time.time()
        audio = f5_tts.synthesize(test_text)
        latency_ms = (time.time() - start_time) * 1000
        
        metrics["latency_ms"] = latency_ms
        metrics["audio_length_samples"] = len(audio)
        metrics["audio_duration_sec"] = len(audio) / 24000
        
        print(f"   –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–∏–Ω—Ç–µ–∑–∞: {latency_ms:.2f} –º—Å")
        print(f"   –î–ª–∏–Ω–∞ –∞—É–¥–∏–æ: {len(audio)} —Å—ç–º–ø–ª–æ–≤ ({metrics['audio_duration_sec']:.2f} —Å–µ–∫)")
        
        # –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        del f5_tts
        torch.cuda.empty_cache()
        
        vram_after_unload = self.get_vram_usage()
        metrics["vram_after_unload_mb"] = vram_after_unload
        
        return metrics
    
    def test_llm_model(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen3 –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        import json
        
        project_root = Path(__file__).parent.parent
        model_path = project_root / "models" / "Qwen3-16B-A3B-abliterated-AWQ"
        
        metrics = {}
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ config.json
        config_json = model_path / "config.json"
        if not config_json.exists():
            raise RuntimeError(f"config.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {model_path}")
        
        with open(config_json, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        metrics["model_type"] = config.get("model_type")
        metrics["hidden_size"] = config.get("hidden_size")
        metrics["num_hidden_layers"] = config.get("num_hidden_layers")
        metrics["num_experts"] = config.get("num_experts")
        metrics["num_experts_per_tok"] = config.get("num_experts_per_tok")
        
        print(f"   –¢–∏–ø –º–æ–¥–µ–ª–∏: {metrics['model_type']}")
        print(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: MoE —Å {metrics['num_experts']} —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏")
        print(f"   –≠–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω: {metrics['num_experts_per_tok']}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
        model_files = list(model_path.glob("*.safetensors")) + list(model_path.glob("*.bin"))
        metrics["model_files_count"] = len(model_files)
        
        if not model_files:
            raise RuntimeError(f"–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {model_path}")
        
        print(f"   –§–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏: {len(model_files)}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ vLLM –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å (–Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º, —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å)
        metrics["model_path"] = str(model_path)
        print(f"   –ü—É—Ç—å –º–æ–¥–µ–ª–∏: {model_path} ‚úÖ")
        
        return metrics
    
    async def test_vram_all_models(self) -> Dict[str, Any]:
        """–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º VRAM."""
        import subprocess
        import signal
        
        metrics = {}
        vram_snapshots = []
        
        vram_initial = self.get_vram_usage()
        vram_snapshots.append({"stage": "initial", "vram_mb": vram_initial})
        metrics["vram_initial_mb"] = vram_initial
        
        print(f"   –ù–∞—á–∞–ª—å–Ω—ã–π VRAM: {vram_initial:.0f} MB")
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ ASR
        print("\n   1. –ó–∞–≥—Ä—É–∑–∫–∞ ASR (faster-whisper)...")
        from faster_whisper import WhisperModel
        asr_model = WhisperModel("dropbox-dash/faster-whisper-large-v3-turbo", device="cuda", compute_type="int8_float16")
        vram_asr = self.get_vram_usage()
        vram_snapshots.append({"stage": "asr_loaded", "vram_mb": vram_asr})
        metrics["vram_asr_mb"] = vram_asr
        print(f"      VRAM –ø–æ—Å–ª–µ ASR: {vram_asr:.0f} MB (Œî {vram_asr - vram_initial:+.0f} MB)")
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ TTS
        print("\n   2. –ó–∞–≥—Ä—É–∑–∫–∞ TTS (F5-TTS)...")
        from src.tts_gateway.f5_tts_engine import F5TTSEngine
        import logging
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.WARNING)
        
        project_root = Path(__file__).parent.parent
        f5_tts = F5TTSEngine(
            model_path=str(project_root / "models" / "F5-tts"),
            device="cuda",
            sample_rate=24000,
            use_stress_marks=True,
            logger=logger,
        )
        vram_tts = self.get_vram_usage()
        vram_snapshots.append({"stage": "tts_loaded", "vram_mb": vram_tts})
        metrics["vram_tts_mb"] = vram_tts
        print(f"      VRAM –ø–æ—Å–ª–µ TTS: {vram_tts:.0f} MB (Œî {vram_tts - vram_asr:+.0f} MB)")
        
        # 3. –ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞ (–≤ —Ñ–æ–Ω–µ)
        print("\n   3. –ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞...")
        print("      (–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–µ—Ä–≤–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω –∏–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä
        vllm_running = False
        try:
            async def check_vllm():
                async with httpx.AsyncClient(timeout=2.0) as client:
                    response = await client.get("http://localhost:8000/v1/models")
                    return response.status_code == 200
            vllm_running = asyncio.run(check_vllm())
        except:
            pass
        
        if not vllm_running:
            print("      ‚ö†Ô∏è  vLLM —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ:")
            print("         vllm serve models/Qwen3-16B-A3B-abliterated-AWQ --host 0.0.0.0 --port 8000 --quantization awq")
            metrics["vllm_warning"] = "vLLM —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω"
        else:
            print("      ‚úÖ vLLM —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")
            # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ —á—Ç–æ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è
            await asyncio.sleep(5)
        
        vram_vllm = self.get_vram_usage()
        vram_snapshots.append({"stage": "vllm_loaded", "vram_mb": vram_vllm})
        metrics["vram_vllm_mb"] = vram_vllm
        print(f"      VRAM –ø–æ—Å–ª–µ vLLM: {vram_vllm:.0f} MB (Œî {vram_vllm - vram_tts:+.0f} MB)")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_vram_used = vram_vllm - vram_initial
        metrics["vram_total_used_mb"] = total_vram_used
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—â—É—é VRAM GPU
        if torch.cuda.is_available():
            total_vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
            metrics["vram_total_gb"] = total_vram_gb
            free_vram_gb = (torch.cuda.get_device_properties(0).total_memory - vram_vllm * 1024 ** 2) / (1024 ** 3)
            metrics["vram_free_gb"] = free_vram_gb
            
            print(f"\n   üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"      –í—Å–µ–≥–æ VRAM: {total_vram_gb:.2f} GB")
            print(f"      –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {total_vram_used / 1024:.2f} GB")
            print(f"      –°–≤–æ–±–æ–¥–Ω–æ: {free_vram_gb:.2f} GB")
            
            if free_vram_gb < 1.0:
                metrics["warning"] = f"–ú–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–π VRAM: {free_vram_gb:.2f} GB (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 1 GB)"
                print(f"      ‚ö†Ô∏è  {metrics['warning']}")
        
        # –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        print("\n   –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        del asr_model
        del f5_tts
        torch.cuda.empty_cache()
        
        vram_final = self.get_vram_usage()
        vram_snapshots.append({"stage": "unloaded", "vram_mb": vram_final})
        metrics["vram_final_mb"] = vram_final
        metrics["vram_snapshots"] = vram_snapshots
        
        return metrics
    
    async def test_redis(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏ —Ä–∞–±–æ—Ç—ã Redis."""
        import redis.asyncio as aioredis
        import os
        
        metrics = {}
        
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        metrics["redis_url"] = redis_url
        
        print(f"   –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis: {redis_url}...")
        
        try:
            client = await aioredis.from_url(redis_url, encoding="utf-8", decode_responses=True)
            
            # Ping
            pong = await client.ping()
            if not pong:
                raise RuntimeError("Redis ping failed")
            metrics["ping_success"] = True
            print("   ‚úÖ Ping —É—Å–ø–µ—à–µ–Ω")
            
            # –¢–µ—Å—Ç –∑–∞–ø–∏—Å–∏/—á—Ç–µ–Ω–∏—è
            test_key = "test_system_check"
            test_value = "test_value_123"
            await client.set(test_key, test_value, ex=10)
            read_value = await client.get(test_key)
            
            if read_value != test_value:
                raise RuntimeError(f"Redis read/write test failed: expected {test_value}, got {read_value}")
            
            await client.delete(test_key)
            metrics["read_write_success"] = True
            print("   ‚úÖ –ó–∞–ø–∏—Å—å/—á—Ç–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ Redis
            info = await client.info("memory")
            metrics["redis_memory_used_mb"] = int(info.get("used_memory", 0)) / (1024 ** 2)
            print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ Redis: {metrics['redis_memory_used_mb']:.2f} MB")
            
            await client.close()
            
        except Exception as e:
            raise RuntimeError(f"Redis connection failed: {e}") from e
        
        return metrics
    
    async def test_vllm_server(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ vLLM —Å–µ—Ä–≤–µ—Ä–∞ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º TTFT –∏ latency."""
        from openai import AsyncOpenAI
        
        metrics = {}
        
        base_url = "http://localhost:8000/v1"
        print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ vLLM —Å–µ—Ä–≤–µ—Ä–∞: {base_url}...")
        
        client = AsyncOpenAI(base_url=base_url, api_key="EMPTY")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω
        try:
            models = await client.models.list()
            if not models.data:
                raise RuntimeError("vLLM —Å–µ—Ä–≤–µ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª –º–æ–¥–µ–ª–∏")
            
            model_name = models.data[0].id
            metrics["model_name"] = model_name
            print(f"   ‚úÖ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω, –º–æ–¥–µ–ª—å: {model_name}")
            
        except Exception as e:
            raise RuntimeError(f"vLLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}") from e
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        test_messages = [
            {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫."},
            {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –°–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫–æ: –∫–∞–∫ –¥–µ–ª–∞?"}
        ]
        
        print("   –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è...")
        start_time = time.time()
        
        response = await client.chat.completions.create(
            model=model_name,
            messages=test_messages,
            max_tokens=50,
            temperature=0.7,
        )
        
        total_time = time.time() - start_time
        ttft_ms = response.response_headers.get("x-first-token-ms") if hasattr(response, 'response_headers') else None
        
        metrics["total_latency_ms"] = total_time * 1000
        metrics["ttft_ms"] = float(ttft_ms) if ttft_ms else None
        metrics["response_text"] = response.choices[0].message.content[:50]
        metrics["tokens_generated"] = response.usage.completion_tokens if hasattr(response.usage, 'completion_tokens') else None
        
        print(f"   ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞")
        print(f"      –û–±—â–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {metrics['total_latency_ms']:.2f} –º—Å")
        if metrics["ttft_ms"]:
            print(f"      TTFT: {metrics['ttft_ms']:.2f} –º—Å")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ VRAM vLLM
        vram_vllm = self.get_vram_usage()
        metrics["vram_usage_mb"] = vram_vllm
        print(f"      VRAM: {vram_vllm:.0f} MB")
        
        return metrics
    
    async def test_asr_gateway(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ ASR Gateway health –∏ readiness."""
        metrics = {}
        
        base_url = "http://localhost:8001"
        print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ ASR Gateway: {base_url}...")
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            # Health check
            try:
                response = await client.get(f"{base_url}/health")
                if response.status_code != 200:
                    raise RuntimeError(f"Health check failed: HTTP {response.status_code}")
                metrics["health_status"] = "ok"
                print("   ‚úÖ Health check —É—Å–ø–µ—à–µ–Ω")
            except Exception as e:
                raise RuntimeError(f"ASR Gateway –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}") from e
            
            # Readiness check
            try:
                response = await client.get(f"{base_url}/ready")
                metrics["ready"] = response.status_code == 200
                print(f"   ‚úÖ Readiness: {'ready' if metrics['ready'] else 'not ready'}")
            except Exception as e:
                metrics["ready"] = False
                metrics["ready_error"] = str(e)
                print(f"   ‚ö†Ô∏è  Readiness check failed: {e}")
        
        return metrics
    
    async def test_tts_gateway(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ TTS Gateway —Å —Ç–µ—Å—Ç–æ–≤—ã–º —Å–∏–Ω—Ç–µ–∑–æ–º."""
        metrics = {}
        
        base_url = "http://localhost:8002"
        print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ TTS Gateway: {base_url}...")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Health check
            try:
                response = await client.get(f"{base_url}/health")
                if response.status_code != 200:
                    raise RuntimeError(f"Health check failed: HTTP {response.status_code}")
                metrics["health_status"] = "ok"
                print("   ‚úÖ Health check —É—Å–ø–µ—à–µ–Ω")
            except Exception as e:
                raise RuntimeError(f"TTS Gateway –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}") from e
            
            # –¢–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑
            test_text = "–î–æ–±—Ä—ã–π –¥–µ–Ω—å!"
            print(f"   –¢–µ—Å—Ç–æ–≤—ã–π —Å–∏–Ω—Ç–µ–∑: '{test_text}'...")
            
            start_time = time.time()
            response = await client.post(
                f"{base_url}/synthesize",
                json={"text": test_text},
            )
            latency_ms = (time.time() - start_time) * 1000
            
            if response.status_code != 200:
                raise RuntimeError(f"Synthesis failed: HTTP {response.status_code}")
            
            audio_data = response.content
            sample_rate = int(response.headers.get("X-Sample-Rate", "24000"))
            channels = int(response.headers.get("X-Channels", "1"))
            
            metrics["latency_ms"] = latency_ms
            metrics["audio_size_bytes"] = len(audio_data)
            metrics["sample_rate"] = sample_rate
            metrics["channels"] = channels
            
            print(f"   ‚úÖ –°–∏–Ω—Ç–µ–∑ —É—Å–ø–µ—à–µ–Ω")
            print(f"      –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {latency_ms:.2f} –º—Å")
            print(f"      –†–∞–∑–º–µ—Ä –∞—É–¥–∏–æ: {len(audio_data)} –±–∞–π—Ç")
            print(f"      Sample rate: {sample_rate} Hz")
        
        return metrics
    
    async def test_policy_engine(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ Policy Engine —Å —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∏–∞–ª–æ–≥–æ–º."""
        metrics = {}
        
        base_url = "http://localhost:8003"
        print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ Policy Engine: {base_url}...")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Health check
            try:
                response = await client.get(f"{base_url}/health")
                if response.status_code != 200:
                    raise RuntimeError(f"Health check failed: HTTP {response.status_code}")
                metrics["health_status"] = "ok"
                print("   ‚úÖ Health check —É—Å–ø–µ—à–µ–Ω")
            except Exception as e:
                raise RuntimeError(f"Policy Engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}") from e
            
            # –¢–µ—Å—Ç–æ–≤—ã–π –¥–∏–∞–ª–æ–≥
            test_session_id = f"test-{int(time.time())}"
            test_message = "–î–æ–±—Ä—ã–π –¥–µ–Ω—å, —Ö–æ—á—É –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ú–†–¢"
            
            print(f"   –¢–µ—Å—Ç–æ–≤—ã–π –¥–∏–∞–ª–æ–≥ –∑–∞–ø—Ä–æ—Å...")
            start_time = time.time()
            
            response = await client.post(
                f"{base_url}/dialog",
                json={
                    "session_id": test_session_id,
                    "user_message": test_message,
                },
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            if response.status_code != 200:
                raise RuntimeError(f"Dialog request failed: HTTP {response.status_code}")
            
            data = response.json()
            metrics["latency_ms"] = latency_ms
            metrics["agent_message"] = data.get("agent_message", "")[:50]
            metrics["current_state"] = data.get("current_state")
            metrics["is_complete"] = data.get("is_complete", False)
            metrics["slots_count"] = len(data.get("slots", {}))
            
            print(f"   ‚úÖ –î–∏–∞–ª–æ–≥ —É—Å–ø–µ—à–µ–Ω")
            print(f"      –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {latency_ms:.2f} –º—Å")
            print(f"      –°–æ—Å—Ç–æ—è–Ω–∏–µ: {metrics['current_state']}")
            print(f"      –°–ª–æ—Ç–æ–≤ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ: {metrics['slots_count']}")
        
        return metrics
    
    async def test_freeswitch_bridge(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ FreeSWITCH Bridge —á–µ—Ä–µ–∑ API –∏ WebSocket."""
        metrics = {}
        
        base_url = "http://localhost:8004"
        print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ FreeSWITCH Bridge: {base_url}...")
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            # Health check
            try:
                response = await client.get(f"{base_url}/health")
                if response.status_code != 200:
                    raise RuntimeError(f"Health check failed: HTTP {response.status_code}")
                metrics["health_status"] = "ok"
                print("   ‚úÖ Health check —É—Å–ø–µ—à–µ–Ω")
            except Exception as e:
                raise RuntimeError(f"FreeSWITCH Bridge –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}") from e
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ API –∫–ª—é—á–∞
            from src.shared.config_loader import load_and_validate_config
            from pathlib import Path
            
            config_path = Path(__file__).parent.parent / "src" / "freeswitch_bridge" / "config.yaml"
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω—Ñ–∏–≥ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
            if config_path.exists():
                metrics["config_exists"] = True
                print("   ‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞")
            else:
                metrics["config_exists"] = False
                print("   ‚ö†Ô∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        # WebSocket —Ç–µ—Å—Ç (—Å–∏–º—É–ª—è—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è)
        try:
            import websockets
            ws_url = f"ws://localhost:8004/ws"
            print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ WebSocket: {ws_url}...")
            
            async def test_ws():
                try:
                    async with websockets.connect(ws_url, timeout=2.0) as ws:
                        await ws.ping()
                        return True
                except:
                    return False
            
            ws_available = await test_ws()
            metrics["websocket_available"] = ws_available
            if ws_available:
                print("   ‚úÖ WebSocket –¥–æ—Å—Ç—É–ø–µ–Ω")
            else:
                print("   ‚ö†Ô∏è  WebSocket –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –µ—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–≤–æ–Ω–∫–æ–≤)")
        except ImportError:
            metrics["websocket_test"] = "websockets not installed"
            print("   ‚ö†Ô∏è  websockets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º WebSocket —Ç–µ—Å—Ç")
        except Exception as e:
            metrics["websocket_error"] = str(e)
            print(f"   ‚ö†Ô∏è  WebSocket —Ç–µ—Å—Ç failed: {e}")
        
        return metrics
    
    async def test_e2e_dialog(self) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π E2E —Ç–µ—Å—Ç –¥–∏–∞–ª–æ–≥–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞."""
        from scripts.simulate_dialog import DialogSimulator
        
        metrics = {}
        
        print("   –ó–∞–ø—É—Å–∫ E2E —Ç–µ—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞...")
        
        simulator = DialogSimulator(
            policy_url="http://localhost:8003",
            session_id=f"e2e-test-{int(time.time())}",
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–æ–≤ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
        if not await simulator.check_services():
            raise RuntimeError("Policy Engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è E2E —Ç–µ—Å—Ç–∞")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–∏–∞–ª–æ–≥ (5-10 —Ö–æ–¥–æ–≤)
        start_time = time.time()
        
        try:
            # –ü–µ—Ä–≤—ã–π —Ö–æ–¥
            agent_msg, state, is_complete, slots = await simulator.send_message("")
            metrics["first_turn_latency_ms"] = simulator.metrics.avg_response_time_ms
            
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Ö–æ–¥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
            test_messages = [
                "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤",
                "–£ –º–µ–Ω—è –±–æ–ª–∏—Ç –≥–æ–ª–æ–≤–∞",
                "–•–æ—á—É –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∑–∞–≤—Ç—Ä–∞ –≤ 15:00",
            ]
            
            turn_latencies = []
            for msg in test_messages:
                turn_start = time.time()
                agent_msg, state, is_complete, slots = await simulator.send_message(msg)
                turn_latency = (time.time() - turn_start) * 1000
                turn_latencies.append(turn_latency)
            
            total_time = time.time() - start_time
            
            metrics["total_turns"] = simulator.metrics.total_turns
            metrics["total_time_seconds"] = total_time
            metrics["avg_turn_latency_ms"] = sum(turn_latencies) / len(turn_latencies) if turn_latencies else 0
            metrics["turn_latencies_ms"] = turn_latencies
            metrics["fsm_transitions"] = simulator.metrics.fsm_transitions
            metrics["slots_filled"] = simulator.metrics.slots_filled
            metrics["is_complete"] = is_complete
            
            print(f"   ‚úÖ E2E —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")
            print(f"      –•–æ–¥–æ–≤: {metrics['total_turns']}")
            print(f"      –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
            print(f"      –°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ö–æ–¥–∞: {metrics['avg_turn_latency_ms']:.2f} –º—Å")
            print(f"      –°–ª–æ—Ç–æ–≤ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ: {metrics['slots_filled']}")
            
        except Exception as e:
            raise RuntimeError(f"E2E —Ç–µ—Å—Ç failed: {e}") from e
        
        return metrics
    
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = Path(__file__).parent.parent / "test_reports"
        report_dir.mkdir(exist_ok=True)
        
        md_path = report_dir / f"test_report_{timestamp}.md"
        json_path = report_dir / f"test_report_{timestamp}.json"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON –æ—Ç—á–µ—Ç
        json_data = {
            "timestamp": datetime.now().isoformat(),
            "duration_seconds": time.time() - self.start_time,
            "results": {name: asdict(result) for name, result in self.results.items()},
            "vram_snapshots": self.vram_snapshots,
        }
        
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Markdown –æ—Ç—á–µ—Ç
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# Sales Agent - Test Report\n\n")
            f.write(f"**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {time.time() - self.start_time:.2f} —Å–µ–∫—É–Ω–¥\n\n")
            
            # –°–≤–æ–¥–∫–∞
            total = len(self.results)
            success = sum(1 for r in self.results.values() if r.status == "success")
            errors = sum(1 for r in self.results.values() if r.status == "error")
            
            f.write(f"## –°–≤–æ–¥–∫–∞\n\n")
            f.write(f"- –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–æ–∫: {total}\n")
            f.write(f"- –£—Å–ø–µ—à–Ω–æ: {success} ‚úÖ\n")
            f.write(f"- –û—à–∏–±–æ–∫: {errors} ‚ùå\n\n")
            
            # –¢–∞–±–ª–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è VRAM
            if self.vram_snapshots:
                f.write(f"## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM\n\n")
                f.write(f"| –≠—Ç–∞–ø | VRAM (MB) | Œî (MB) |\n")
                f.write(f"|------|-----------|--------|\n")
                prev_vram = None
                for snapshot in self.vram_snapshots:
                    vram = snapshot.get("vram_mb")
                    delta = snapshot.get("vram_delta_mb")
                    if vram:
                        delta_str = f"{delta:+.0f}" if delta else "-"
                        f.write(f"| {snapshot.get('step', 'unknown')} | {vram:.0f} | {delta_str} |\n")
                        prev_vram = vram
                f.write(f"\n")
            
            # –î–µ—Ç–∞–ª–∏ –ø–æ —à–∞–≥–∞–º
            f.write(f"## –î–µ—Ç–∞–ª–∏ –ø—Ä–æ–≤–µ—Ä–æ–∫\n\n")
            for name, result in self.results.items():
                status_emoji = "‚úÖ" if result.status == "success" else "‚ùå"
                f.write(f"### {status_emoji} {name}\n\n")
                f.write(f"- **–°—Ç–∞—Ç—É—Å:** {result.status}\n")
                f.write(f"- **–í—Ä–µ–º—è:** {result.duration_seconds:.2f}s\n")
                
                if result.vram_before_mb and result.vram_after_mb:
                    delta = result.vram_after_mb - result.vram_before_mb
                    f.write(f"- **VRAM:** {result.vram_before_mb:.0f} MB ‚Üí {result.vram_after_mb:.0f} MB (Œî {delta:+.0f} MB)\n")
                    if result.vram_peak_mb:
                        f.write(f"- **VRAM –ø–∏–∫:** {result.vram_peak_mb:.0f} MB\n")
                
                if result.metrics:
                    f.write(f"- **–ú–µ—Ç—Ä–∏–∫–∏:**\n")
                    for k, v in result.metrics.items():
                        if isinstance(v, (int, float)):
                            f.write(f"  - `{k}`: {v}\n")
                        elif isinstance(v, str):
                            f.write(f"  - `{k}`: {v}\n")
                        elif isinstance(v, list):
                            f.write(f"  - `{k}`: {len(v)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n")
                        else:
                            f.write(f"  - `{k}`: {v}\n")
                
                if result.warnings:
                    f.write(f"- **–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:**\n")
                    for warning in result.warnings:
                        f.write(f"  - ‚ö†Ô∏è {warning}\n")
                
                if result.error:
                    f.write(f"- **–û—à–∏–±–∫–∞:** `{result.error}`\n")
                
                f.write(f"\n")
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if errors > 0:
                f.write(f"## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n")
                f.write(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ø—Ä–æ–≤–µ—Ä–∫–∞—Ö:\n\n")
                for name, result in self.results.items():
                    if result.status == "error":
                        f.write(f"- **{name}**: {result.error}\n")
                f.write(f"\n")
            
            # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
            f.write(f"## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏\n\n")
            if errors == 0:
                f.write(f"‚úÖ –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.\n\n")
            else:
                f.write(f"‚ö†Ô∏è –ò—Å–ø—Ä–∞–≤—å—Ç–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã.\n\n")
            
            f.write(f"–î–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤:\n")
            f.write(f"```bash\n")
            f.write(f"uv run python scripts/test_system.py\n")
            f.write(f"```\n")
        
        print(f"\n{'='*70}")
        print(f"üìä –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   Markdown: {md_path}")
        print(f"   JSON: {json_path}")
        print(f"{'='*70}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(description="–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Sales Agent")
    parser.add_argument(
        "--continue-on-error",
        action="store_true",
        help="–ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö",
    )
    parser.add_argument(
        "--steps",
        type=str,
        help="–°–ø–∏—Å–æ–∫ —à–∞–≥–æ–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä: environment,models,vram)",
    )
    
    args = parser.parse_args()
    
    steps = None
    if args.steps:
        steps = [s.strip() for s in args.steps.split(",")]
    
    tester = SystemTester(continue_on_error=args.continue_on_error)
    tester.run_all_tests(steps=steps)


if __name__ == "__main__":
    main()

