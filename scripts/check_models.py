#!/usr/bin/env python3

"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ –≤–µ—Å–æ–≤.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import os
from dotenv import load_dotenv

load_dotenv()


def check_faster_whisper():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ faster-whisper."""
    print("üì• –ü—Ä–æ–≤–µ—Ä–∫–∞ faster-whisper large-v3-turbo (Dropbox)...")
    try:
        from faster_whisper import WhisperModel
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (—Å–∫–∞—á–∞–µ—Ç—Å—è –µ—Å–ª–∏ –Ω–µ—Ç)
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)...")
        model = WhisperModel(
            "dropbox-dash/faster-whisper-large-v3-turbo",
            device="cuda",
            compute_type="int8_float16",
        )
        
        print("   ‚úÖ faster-whisper –≥–æ—Ç–æ–≤")
        print(f"      Device: cuda")
        print(f"      Model: dropbox-dash/faster-whisper-large-v3-turbo")
        del model
        return True
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        return False


def check_silero_vad():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ Silero VAD."""
    print("\nüì• –ü—Ä–æ–≤–µ—Ä–∫–∞ Silero VAD...")
    try:
        import torch
        
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False,
        )
        
        print("   ‚úÖ Silero VAD –≥–æ—Ç–æ–≤")
        return True
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        return False


def check_f5_tts():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ F5-TTS."""
    print("\nüì• –ü—Ä–æ–≤–µ—Ä–∫–∞ F5-TTS...")
    try:
        from src.tts_gateway.f5_tts_engine import F5TTSEngine
        import logging
        
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.WARNING)  # –£–º–µ–Ω—å—à–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        
        print("   –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è F5-TTS (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)...")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        project_root = Path(__file__).parent.parent
        model_path = project_root / "models" / "F5-tts"
        
        f5_tts = F5TTSEngine(
            model_path=str(model_path),
            device="cuda",
            sample_rate=24000,
            use_stress_marks=True,
            logger=logger,
        )
        
        print("   ‚úÖ F5-TTS –≥–æ—Ç–æ–≤")
        print(f"      Model: F5-TTS_RUSSIAN")
        print(f"      Device: cuda")
        print(f"      Sample rate: 24000 Hz")
        del f5_tts
        return True
        
    except ImportError:
        print("   ‚ùå F5-TTS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("      –£—Å—Ç–∞–Ω–æ–≤–∏: pip install f5-tts ruaccent")
        return False
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        return False


def check_qwen_availability():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Qwen3-16B-A3B-abliterated-AWQ."""
    print("\nüì• –ü—Ä–æ–≤–µ—Ä–∫–∞ Qwen3-16B-A3B-abliterated-AWQ...")
    
    project_root = Path(__file__).parent.parent
    model_path = project_root / "models" / "Qwen3-16B-A3B-abliterated-AWQ"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    if model_path.exists():
        config_json = model_path / "config.json"
        if config_json.exists():
            model_files = list(model_path.glob("*.safetensors")) + list(model_path.glob("*.bin"))
            if model_files:
                size_gb = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file()) / (1024**3)
                print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ: {model_path}")
                print(f"      –†–∞–∑–º–µ—Ä: {size_gb:.2f} GB")
                print(f"      –§–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏: {len(model_files)}")
                return True
    
    # Fallback: –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤ HuggingFace cache
    cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
    model_name = "models--warshanks--Qwen3-16B-A3B-abliterated-AWQ"
    model_cache = cache_dir / model_name
    
    if model_cache.exists():
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ: {model_cache}")
        size_gb = sum(f.stat().st_size for f in model_cache.rglob('*') if f.is_file()) / (1024**3)
        print(f"      –†–∞–∑–º–µ—Ä: {size_gb:.2f} GB")
        return True
    else:
        print(f"   ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print(f"      –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å: {model_path}")
        print("      –°–∫–∞—á–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ vLLM (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ ~15-20 –º–∏–Ω—É—Ç)")
        return False


def check_cuda():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA."""
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA...")
    try:
        import torch
        
        if torch.cuda.is_available():
            print(f"   ‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞")
            print(f"      Devices: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"      GPU {i}: {props.name}")
                print(f"         VRAM: {props.total_memory / (1024**3):.2f} GB")
            return True
        else:
            print("   ‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return False
            
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        return False


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("=" * 70)
    print(" " * 20 + "Sales Agent - Model Checker")
    print("=" * 70)
    print()
    
    results = {}
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    results["cuda"] = check_cuda()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π
    results["faster_whisper"] = check_faster_whisper()
    results["silero_vad"] = check_silero_vad()
    results["f5_tts"] = check_f5_tts()
    results["qwen"] = check_qwen_availability()
    
    # –ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å
    print()
    print("=" * 70)
    print("–ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å:")
    print("=" * 70)
    
    for name, status in results.items():
        emoji = "‚úÖ" if status else "‚ö†Ô∏è"
        print(f"  {emoji} {name:.<30} {'OK' if status else 'Not ready'}")
    
    print()
    
    critical = ["cuda", "faster_whisper", "f5_tts"]
    if all(results.get(k, False) for k in critical):
        print("‚úÖ –í—Å–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã –∫ —Ä–∞–±–æ—Ç–µ!")
        print()
        print("–ú–æ–∂–µ—à—å –∑–∞–ø—É—Å–∫–∞—Ç—å —Å–µ—Ä–≤–∏—Å—ã:")
        print("  1. vLLM: vllm serve models/Qwen3-16B-A3B-abliterated-AWQ --host 0.0.0.0 --port 8000 --quantization awq --enable-chunked-prefill --enable-prefix-caching")
        print("  2. ASR Gateway: uv run python src/asr_gateway/main.py")
        print("  3. TTS Gateway: uv run python src/tts_gateway/main.py")
        print("  4. Policy Engine: uv run python src/policy_engine/main.py")
        print("  5. FreeSWITCH Bridge: uv run python src/freeswitch_bridge/main.py")
    else:
        print("‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ –≥–æ—Ç–æ–≤—ã.")
        print()
        print("–£—Å—Ç–∞–Ω–æ–≤–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ –ø–æ–≤—Ç–æ—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫—É.")


if __name__ == "__main__":
    main()

