# ======================================
# LLM Service Configuration
# ======================================

# Модель
model:
  name: "Qwen/Qwen2.5-14B-Instruct-AWQ"
  quantization: "awq"
  max_model_len: 2048
  gpu_memory_utilization: 0.75
  
# vLLM сервер
server:
  host: "0.0.0.0"
  port: 8000
  api_key: null  # Опционально для защиты API
  
# Параметры генерации (по умолчанию)
generation:
  temperature: 0.7
  top_p: 0.9
  max_tokens: 512
  # Structured output для слотов
  response_format_type: "json_object"  # Для structured output
  
# Performance
performance:
  enable_chunked_prefill: true
  max_num_batched_tokens: 2048
  # Flash Attention 2 включается автоматически если доступен
  enable_prefix_caching: true
  
# Guard checks
guards:
  require_cuda: true
  min_vram_mb: 8192  # 14B AWQ занимает ~8GB

