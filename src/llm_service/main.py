"""
LLM Service - FastAPI –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è vLLM —Å OpenAI-compatible API.

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–ø—É—Å–∫–∞–µ—Ç vLLM —Å–µ—Ä–≤–µ—Ä –æ—Ç–¥–µ–ª—å–Ω–æ.
–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç helper-—Ñ—É–Ω–∫—Ü–∏–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –∑–∞–ø—É—Å–∫–∞.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import os
from dotenv import load_dotenv
from pydantic import BaseModel
from pydantic_settings import BaseSettings
from typing import Optional, Dict, Any, List
from openai import AsyncOpenAI

from src.shared.logging_config import setup_logging
from src.shared.config_loader import load_and_validate_config

load_dotenv()

logger = setup_logging("llm_service")


class LLMConfig(BaseSettings):
    """Pydantic Settings –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LLM."""
    
    class ModelConfig(BaseModel):
        name: str
        quantization: str
        max_model_len: int
        gpu_memory_utilization: float
    
    class ServerConfig(BaseModel):
        host: str
        port: int
        api_key: Optional[str] = None
    
    class GenerationConfig(BaseModel):
        temperature: float
        top_p: float
        max_tokens: int
        response_format_type: str
    
    class PerformanceConfig(BaseModel):
        enable_chunked_prefill: bool
        max_num_batched_tokens: int
        enable_prefix_caching: bool
    
    class GuardsConfig(BaseModel):
        require_cuda: bool
        min_vram_mb: int
    
    model: ModelConfig
    server: ServerConfig
    generation: GenerationConfig
    performance: PerformanceConfig
    guards: GuardsConfig


class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å vLLM —Å–µ—Ä–≤–µ—Ä–æ–º."""

    def __init__(self, base_url: str, api_key: Optional[str] = None, model_name: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞.
        
        Args:
            base_url: URL vLLM —Å–µ—Ä–≤–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, http://localhost:8000/v1)
            api_key: API –∫–ª—é—á (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –≤ vLLM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ)
        """
        self.client = AsyncOpenAI(
            base_url=base_url,
            api_key=api_key or "EMPTY",
        )
        self.logger = logger
        self.model_name = model_name  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö

    async def generate(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 512,
        response_format: Optional[Dict[str, str]] = None,
        model_name: Optional[str] = None,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            response_format: –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è structured output)
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Raises:
            Exception: –ü—Ä–∏ –æ—à–∏–±–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        try:
            # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω JSON –æ—Ç–≤–µ—Ç
            extra_body = {}
            if response_format and response_format.get("type") == "json_object":
                extra_body["response_format"] = response_format

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ
            model = model_name or self.model_name or "models/Qwen3-16B-A3B-abliterated-AWQ"

            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                extra_body=extra_body if extra_body else None,
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            self.logger.error(f"LLM generation error: {e}", exc_info=True)
            raise

    async def generate_structured(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.3,  # –ù–∏–∂–µ –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ JSON
        max_tokens: int = 512,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç structured JSON –æ—Ç–≤–µ—Ç.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            JSON —Å—Ç—Ä–æ–∫–∞
        """
        return await self.generate(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format={"type": "json_object"},
        )

    async def health_check(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å vLLM —Å–µ—Ä–≤–µ—Ä–∞.
        
        Returns:
            True –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            models = await self.client.models.list()
            return len(models.data) > 0
        except Exception as e:
            self.logger.error(f"Health check failed: {e}")
            return False


def get_vllm_launch_command(config: LLMConfig) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ vLLM —Å–µ—Ä–≤–µ—Ä–∞.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM
        
    Returns:
        –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ vLLM
    """
    cmd = [
        "vllm", "serve",
        config.model.name,
        f"--host {config.server.host}",
        f"--port {config.server.port}",
        f"--max-model-len {config.model.max_model_len}",
        f"--gpu-memory-utilization {config.model.gpu_memory_utilization}",
        f"--quantization {config.model.quantization}",
    ]
    
    if config.performance.enable_chunked_prefill:
        cmd.append("--enable-chunked-prefill")
    
    if config.performance.enable_prefix_caching:
        cmd.append("--enable-prefix-caching")
    
    if config.performance.max_num_batched_tokens:
        cmd.append(f"--max-num-batched-tokens {config.performance.max_num_batched_tokens}")
    
    return " \\\n  ".join(cmd)


def main():
    """
    –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ - –≤—ã–≤–æ–¥–∏—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∑–∞–ø—É—Å–∫—É vLLM.
    """
    print("=" * 60)
    print("LLM Service - vLLM Server Launch Instructions")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config_path = Path(__file__).parent / "config.yaml"
    config = load_and_validate_config(config_path, LLMConfig, "LLM_SERVICE")
    
    print("\nüöÄ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–æ–ª–∂–µ–Ω –∑–∞–ø—É—Å—Ç–∏—Ç—å vLLM —Å–µ—Ä–≤–µ—Ä —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ.\n")
    print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞:\n")
    
    cmd = get_vllm_launch_command(config)
    print(f"  {cmd}\n")
    
    print("\n–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ, —á–µ—Ä–µ–∑ Python API:\n")
    print("""
from vllm import LLM

llm = LLM(
    model="models/Qwen3-16B-A3B-abliterated-AWQ",  # –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    quantization="awq",
    max_model_len=2048,
    gpu_memory_utilization=0.75,
)
""")
    
    print("\n" + "=" * 60)
    print("–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ vLLM:")
    print(f"  - API –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞: http://{config.server.host}:{config.server.port}/v1")
    print(f"  - OpenAI-compatible endpoint: /v1/chat/completions")
    print("=" * 60)


if __name__ == "__main__":
    main()

